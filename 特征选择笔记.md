# 特征选择
- [特征选择](#特征选择)
  - [目的——选择对模型的重要特征](#目的选择对模型的重要特征)
  - [选择方法](#选择方法)
    - [一、过滤法 (Filter)](#一过滤法-filter)
      - [1. 绘图判断](#1-绘图判断)
      - [2. 单特征](#2-单特征)
        - [a. 缺失百分比](#a-缺失百分比)
        - [b. 方差](#b-方差)
        - [c. 频数](#c-频数)
      - [3. 多特征](#3-多特征)
        - [a. 数值特征与数值特征](#a-数值特征与数值特征)
        - [b. 类别特征与类别特征](#b-类别特征与类别特征)
        - [c. 数值特征与类别特征](#c-数值特征与类别特征)
    - [二、包裹法 (Wrapper)](#二包裹法-wrapper)
      - [1. 完全搜索](#1-完全搜索)
      - [2. 启发式搜索](#2-启发式搜索)
        - [a. 向前/向后搜索](#a-向前向后搜索)
        - [b. 递归特征消除](#b-递归特征消除)
      - [3. 随机搜索](#3-随机搜索)
        - [a. 随机特征子集](#a-随机特征子集)
        - [b. Null Importance](#b-null-importance)
    - [三、嵌入法 (Embedded)](#三嵌入法-embedded)
      - [1. 基于惩罚项](#1-基于惩罚项)
        - [a. 回归问题-正则化](#a-回归问题-正则化)
        - [b. 分类问题](#b-分类问题)
      - [2. 基于树模型](#2-基于树模型)
        - [a. 决策树模型 (CART)](#a-决策树模型-cart)
        - [b. 森林模型](#b-森林模型)
        - [c. Adaboost](#c-adaboost)
        - [e. GBDT](#e-gbdt)
- [Sklearn中的方法](#sklearn中的方法)
  - [一、过滤法](#一过滤法)
    - [1. 方差选择法](#1-方差选择法)
    - [2. 单变量特征选择](#2-单变量特征选择)
  - [二、 包囊法](#二-包囊法)
    - [递归特征消除法](#递归特征消除法)
  - [三、嵌入法](#三嵌入法)
    - [1. sklearn的SelectFromModel函数](#1-sklearn的selectfrommodel函数)
    - [2. 基于 L1 的特征选取](#2-基于-l1-的特征选取)
    - [3. 基于树的特征选取](#3-基于树的特征选取)
- [参考资料](#参考资料)


******
******
## 目的——选择对模型的重要特征
优点:
>   * 减少训练模型的数据的大小,降低学习任务难度,减少数据噪声
>   * 去除无关特征[^1]和冗余特征[^2]
>   * 特征数少，有利于解释模型

重要特征(好的特征)：
>   * 有区分性
>   * 特征之间相互独立
>   * 简单易于理解

---

## 选择方法
[**过滤式**](#一过滤法)
[**包囊式**](#二包裹法)
[**嵌入式**](#三嵌入法)

***
### 一、过滤法 (Filter)
***

![过滤式](https://img2020.cnblogs.com/blog/1814235/202012/1814235-20201203214937358-1170279288.png)
先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关，这相当于先用特征选择过程对初识特征进行“过滤”，然后再用过滤后的特征来训练模型。
- 优点: 特征选择计算开销小，且能有效避免过拟合[^3]。
- 缺点: 没考虑针对后续要使用的学习器去选择特征子集，减弱学习器拟合能力。


#### 1. 绘图判断
一般对于强相关性的两个变量，画图就能定性判断是否相关

#### 2. 单特征
##### a. 缺失百分比
缺失样本比例过多且难以填补的特征，建议剔除该变量。
##### b. 方差
若某连续型变量的方差接近于0，说明其特征值趋向于单一值的状态，对模型帮助不大，建议剔除该变量。
##### c. 频数
若某类别型变量的枚举值样本量占比分布，集中在单一某枚举值上，建议剔除该变量。

#### 3. 多特征
研究多变量之间的关系时，主要从两种关系出发：
:   - 自变量与自变量之间的相关性: 相关性越高，会引发多重共线性问题，进而导致模型稳定性变差，样本微小扰动都会带来大的参数变化，建议在具有共线性的特征中选择一个即可，其余剔除。
    - 自变量和因变量之间的相关性: 相关性越高，说明特征对模型预测目标更重要，建议保留。

由于变量分连续型变量和类别型变量，所以在研究变量间关系时，也要选用不同的方法:


##### a. 数值特征与数值特征

* ###### 协方差:
  ![协方差](https://bkimg.cdn.bcebos.com/formula/bd5a49802fd29c7a58bbba490f66ee93.svg)
        如果协方差为正，说明X,Y同向变化，协方差越大说明同向程度越高；
        如果协方差维负，说明X，Y反向运动，协方差越小说明反向程度越高；
        如果两个变量相互独立，那么协方差就是0，说明两个变量不相关。
        
* ###### Pearson系数[^4]:
    相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差。
![皮尔森相关系数](https://img-blog.csdnimg.cn/img_convert/71f19bda786d91f4bf90610526a188d4.png)
可以反映两个变量变化时是同向还是反向，如果同向变化就为正，反向变化就为负。

>由于它是标准化后的协方差，因此更重要的特性来了，它消除了两个变量变化幅度的影响，而只是单纯反应两个变量每单位变化时的相似程度。
>
>相关系数分类：
    >>0.8-1.0 极强相关；
    0.6-0.8 强相关；
    0.4-0.6 中等程度相关；
    0.2-0.4 弱相关；
    0.0-0.2 极弱相关或无相关

优点：可以通过数字对变量的关系进行度量，并且带有方向性，1表示正相关，-1表示负相关，可以对变量关系的强弱进行度量，越靠近0相关性越弱。

缺点：无法利用这种关系对数据进行预测，简单的说就是没有对变量间的关系进行提炼和固化，形成模型。要利用变量间的关系进行预测，需要使用到下一种相关分析方法，回归分析。

使用场景：当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：
>       两个变量之间是线性关系，都是连续数据。
>       两个变量的总体是正态分布，或接近正态的单峰分布。
>       两个变量的观测值是成对的，每对观测值之间相互独立。


* ###### 距离相关系数:
    线性关系可以通过Pearson相关系数来描述，单调关系可以通过spearman或者kendall来描述，非线性如何描述，距离相关系数可以非线性相关性。

    皮尔森关联系数只能描述数据键的线性相关性程度，对于非线性相关数据，皮尔森关联系数显然不适合的，距离相关系数恰恰能在很大程度上客服皮尔森相关系数的弱点。
    如果距离相关系数是0 ，那么我们就可以说这两个变量是独立的

    比如：Pearson相关系数等于0，这两个变量并不一定就是独立的（有可能是非线性相关）；但如果距离相关系数为0的话，那么就可以说这两个变量是独立的了。


##### b. 类别特征与类别特征

* ###### 卡方检验:
    先假设两个变量确实是独立的（“原假设”）,然后观察实际值（观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度。

* ###### Fisher得分:
    在同一个类别中的取值比较相似，而在不同类别之间的取值差异比较大；fisher得分越高，特征在不同类别中的差异性越大，在同一类别中的差异性越小，则特征越重要。

* ###### F检验:
    用来判断特征与label的相关性的，F 检验只能表示线性相关关系


* ###### Spearman系数（分类，类别型与类别型）:
    Pearson相关系数是建立在变量符合正态分布的基础上，而Spearman相关系数不假设变量服从何种分布，它是基于等级(rank)的概念去计算变量间的相关性。如果变量是顺序变量(Ordinal Feature)，推荐使用Spearman相关系数。

    同样地，相关系数趋于1或-1，正负号指向正负相关关系。

* ###### Kendall:
    （肯德尔等级）相关系数（分类）

* ###### 互信息和最大互系数（非参数）:
    互信息：估计特征与label之间的关系。最大信息系数。

* ###### 距离相关系数 [(同上)](#距离相关系数)


##### c. 数值特征与类别特征

* ###### 数值特征离散化:
    将数值特征离散化，然后，使用类别与类别变量相关性分析的方法来分析相关性

* ###### 箱形图:
    使用画箱形图的方法，看类别变量取不同值，数值变量的均值与方差及取值分布情况。

* ###### Relief:
    （Relevant Features）：适用于二分类

* ###### Relief-F:
    适用于多分类


***
### 二、包裹法 (Wrapper)
***

![包囊式](https://img2020.cnblogs.com/blog/1814235/202012/1814235-20201205165936243-54707995.png)
包裹式选择特征（包装法）不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。
- 优点: 包裹式特征选择的目的就是为给定学习器选择最有利于其性能，量身定做的特征子集。
- 缺点: 计算开销大

#### 1. 完全搜索
遍历所有可能组合的特征子集，然后输入模型，选择最佳模型分数的特征子集。不推荐使用，计算开销过大。

#### 2. 启发式搜索
启发式搜索是利用启发式信息不断缩小搜索空间的方法。在特征选择中，模型分数或特征权重可作为启发式信息。

##### a. 向前/向后搜索
向前搜索是先从空集开始，每轮只加入一个特征，然后训练模型，若模型评估分数提高，则保留该轮加入的特征，否则丢弃。反之，向后特征是做减法，先从全特征集开始，每轮减去一个特征，若模型表现减低，则保留特征，否则弃之。

##### b. [递归特征消除](#递归特征消除法)
  * 递归特征消除简称`RFE`(Recursive Feature Elimination)，`RFE`是使用一个基模型进行多轮训练，每轮训练后，消除若干低权值(例特征权重系数或者特征重要性)的特征，再基于新的特征集进行下一轮训练。
  * RFE使用时，要提前限定最后选择的特征数(`n_features_to_select`)，这个超参很难保证一次就设置合理，因为设高了，容易特征冗余，设低了，可能会过滤掉相对重要的特征。而且RFE只是单纯基于特征权重去选择，没有考虑模型表现。
  * 因此RFECV出现了，REFCV是REF + CV(交叉验证)，它的运行机制是：先使用REF获取各个特征的ranking，然后再基于ranking，依次选择[`min_features_to_select`, `len`(feature)]个特征数量的特征子集进行模型训练和交叉验证，最后选择平均分最高的特征子集。

#### 3. 随机搜索
##### a. 随机特征子集
随机选择多个特征子集，然后分别评估模型表现，选择评估分数高的特征子集。

##### b. Null Importance
  * 3年前Kaggle GM Olivier提出Null Importance特征挑选法
  * 多见于标识性强或充满噪声的特征。
  * 举个例子，如果我们把userID作为特征加入模型，预测不同userID属于哪类消费人群，一个过拟合的模型，可以会学到userID到消费人群的直接映射关系(相当于模型直接记住了这个userID是什么消费人群)，那如果我假装把标签打乱，搞个假标签去重新训练预测，我们会发现模型会把userID又直接映射到打乱的标签上，最后真假标签下，userID“见风使舵”地让都自己变成了最重要的特征。我们怎么找出这类特征呢？
  * Olivier的想法很简单：真正强健、稳定且重要的特征一定是在真标签下特征很重要，但一旦标签打乱，这些优质特征的重要性就会变差。相反地，如果某特征在原始标签下表现一般，但打乱标签后，居然重要性上升，明显就不靠谱，这类“见风使舵”的特征就得剔除掉。




***
### 三、嵌入法 (Embedded)
***

![嵌入式](https://img2020.cnblogs.com/blog/1814235/202012/1814235-20201205165640968-2143614251.png)
嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中优化，即在学习器训练过程中自动进行了特征选择。

#### 1. 基于惩罚项

##### a. 回归问题-正则化
* ###### L1 范数正则化 (LASSO)
* ###### L2 范数正则化 (岭回归/Ridge)
##### b. 分类问题
* ###### 逻辑回归 (Logistic Regression)
* ###### 支持向量机 (SVM)

#### 2. 基于树模型
##### a. 决策树模型 (CART)

##### b. 森林模型 
* ###### 随机森林 (RF)
* ###### 极端森林 (ExtraTree)

##### c. Adaboost
##### e. GBDT
* ###### XGboost
* ###### LightGBM

---
---
# Sklearn中的方法
在 `sklearn.feature_selection `模块中的类可以用来对样本集进行 feature selection（特征选择）和 dimensionality reduction（降维）。

---
## 一、过滤法
### 1. 方差选择法
`VarianceThreshold`是特征选择的一个简单基本方法，它会移除所有那些方差不满足一些阈值的特征。默认情况下，它将会移除所有的零方差特征，即那些在所有的样本上的取值均不变的特征。
```py
from sklearn.datasets import load_boston
from sklearn.feature_selection import VarianceThreshold

dat=loda_boston()
x, y = dat.data,dat.target
#用了波士顿房价数据集
print(x.shape) #(506, 13)

sel = VarianceThreshold(threshold=1.5)
# 设置方差阈值为1.5
New=sel.fit_transform(x)

print(New.shape) #(506, 10)
```

### 2. 单变量特征选择
单变量特征选择基于单变量的统计测试来选择最佳特征。它可以看作预测模型的一项预处理。Scikit-learn将特征选择程序用包含transform 函数的对象来展现

>   * `SelectKBest` 移除那些除了评分最高的 K 个特征之外的所有特征
>	* `SelectPercentile` 移除除了用户指定的最高得分百分比之外的所有特征
>    * 对每个特征应用常见的单变量统计测试: 假阳性率（false positive rate） `SelectFpr`, 伪发现率（false discovery rate） `SelectFdr` , 或者族系误差（family wise error） `SelectFwe` 。
>	* `GenericUnivariateSelect` 允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征。同时不同的选择策略也能够使用超参数寻优，从而让我们找到最佳的单变量特征选择策略。

这些作为打分函数输入的对象（同样位于feature_selection模块中），返回单变量的概率值：

*    对于回归: `f_regression`(F检验) , `mutual_info_regression`(互信息)
*    对于分类: `chi2`(卡方) , `f_classif`(F检验) , `mutual_info_classif`(互信息)

```py
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
iris = load_iris()
x, y = iris.data, iris.target
#鸢尾数据集
print(x.shape) #(150, 4)

new = SelectKBest(chi2, k=2).fit_transform(x, y)
#用卡方检验。移除除了k=2个评分最高的特征

print(new.shape) #(150, 2)
```

---
## 二、 包囊法
### 递归特征消除法
>   递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

给定一个外部的估计器，可以对特征赋予一定的权重（比如，线性模型的相关系数），recursive feature elimination ( `RFE` ) 通过考虑越来越小的特征集合来递归的选择特征。 首先，评估器在初始的特征集合上面训练并且每一个特征的重要程度是通过一个 `coef_` 属性 或者 `feature_importances_` 属性来获得。 然后，从当前的特征集合中移除最不重要的特征。在特征集合上不断的重复递归这个步骤，直到最终达到所需要的特征数量为止。 RFECV 在一个交叉验证的循环中执行 `RFE` 来找到最优的特征数量

```py
from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

dat = load_digits()
x, y = dat.data, dat.target
#digits数据集 (这什么东西的集啊？)
print(x.shape) #(1797, 64)

model=LogisticRegression()
#逻辑回归作为基模型的特征选择
New=RFE(model, n_features_to_select=40).fit_transform(x, y)#参数n_features_to_select为选择的特征个数
#递归特征消除法，返回特征选择后的数据

print(New.shape) #(1797, 40)
```

---
## 三、嵌入法
### 1. sklearn的SelectFromModel函数
* SelectFromModel 是一个 meta-transformer（元转换器） ，它可以用来处理任何带有 coef_ 或者 feature_importances_ 属性的训练之后的评估器。 如果相关的coef_ 或者 featureimportances 属性值低于预先设置的阈值，这些特征将会被认为不重要并且移除掉。
* 除了指定数值上的阈值之外，还可以通过给定字符串参数来使用内置的启发式方法找到一个合适的阈值。可以使用的启发式方法有 mean 、 median 以及使用浮点数乘以这些（例如，0.1*mean ）。
```py
class sklearn.feature_selection.SelectFromMode(estimator, threshold=None, prefit=False)
```

### 2. 基于 L1 的特征选取
* Linear models 使用 L1 正则化的线性模型会得到稀疏解：他们的许多系数为 0。 当目标是降低使用另一个分类器的数据集的维度， 它们可以与 `feature_selection.SelectFromModel` 一起使用来选择非零系数。
* 特别的，可以用于此目的的稀疏评估器有
    * 用于回归的 `linear_model.Lasso` 
        * 对于Lasso，参数alpha越大，被选中的特征越少 。
    * 用于分类的 `linear_model.LogisticRegression` 和 `svm.LinearSVC`
        * 对于SVM和逻辑回归，参数C控制稀疏性：C越小，被选中的特征越少。

```py
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel

dat = load_iris()
x, y = dat.data, dat.target
#鸢尾数据集
print(x.shape) #(150, 4)

lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(x, y)
model = SelectFromModel(lsvc, prefit=True)
New = model.transform(x)
#LinearSVC作为基模型的特征选择

print(New_new.shape) #(150, 3)
```

### 3. 基于树的特征选取
* 基于树的预测模型见 `sklearn.tree` 模块
* 基于树的森林预测模型见 `sklearn.ensemble` 模块
* 能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合 `sklearn.feature_selection.SelectFromModel` ）


```py
from sklearn.datasets import load_boston
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import GradientBoostingRegressor

dat=load_boston()
x,y=dat.data,dat.target
#波士顿房价数据集
print(x.shape) #(506, 13)

gbdt = GradientBoostingRegressor()
gbdt.fit(x,y)
New = SelectFromModel(gbdt, prefit=True).transform(x)
#GBDT作为基模型的特征选择

print(New.shape) #(506, 3)
```

* `RandomForestClassifieh`/`ExtraTreesClassifier`自带 `feature_importances_` 功能。能返回特征向量的重要性。

```py
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel

dat = load_iris()
x, y = dat.data, dat.target
#老样子，鸢尾数据集
print(x.shape) #(150, 4)

ET = ExtraTreesClassifier()
ET = ET.fit(x, y)
New = SelectFromModel(ET, prefit=True).transform(x)
#极限森林(分类)作为基模型的特征选择

print(ET.feature_importances_) 
#[0.08703441 0.06055623 0.37836046 0.4740489]
#1,2号特征重要性低`额

print(New.shape) #(150, 2)
```
---


[^1]:"无关特征"是指与当前学习任务无关
[^2]:"冗余特征"：它们所包含的信息能从其他特征中推演出来，例如，考虑立方体对象，若已有特征“底面长”“底面宽”，则“底面积”是冗余特征，因为它能从“底面长”与“底面宽”得到。

[^3]:"过拟合"：一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在训练数据外的数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。

[^4]:"Pearson相关系数":是两个变量的协方差除以两变量的标准差乘积

---

# 参考资料
[Sklearn中文文档](https://www.scikitlearn.com.cn/)
《机器学习》 - 周志华
https://blog.csdn.net/m0_37316673/article/details/107524247
https://blog.csdn.net/Datawhale/article/details/120582526
https://blog.csdn.net/lamusique/article/details/109681764
https://blog.csdn.net/qq_41675973/article/details/103585695
https://blog.csdn.net/weixin_43378396/article/details/90640595
https://www.cnblogs.com/s1awwhy/p/14067489.html
https://www.cnblogs.com/stream886/p/10050058.html
https://www.cnblogs.com/stevenlk/p/6543628.html#24-%E8%B7%9D%E7%A6%BB%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0-distance-correlation